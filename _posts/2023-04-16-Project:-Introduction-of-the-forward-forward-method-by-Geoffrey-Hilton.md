---
published: true
---
Here's a nice video about interview of Geoffrey Hilton on his new Forwarad Forward algorithm, which I am currently working on based on his preliminary research. This interview is pretty cool as it shows us how a great idea was generated: thousands of times of trying, large amount of reading, consistent curiosity and explorations... and also hard working. "To have a great idea have a lot of them!" As Thomas Edison said, 

Video link: [https://www.youtube.com/watch?v=NWqy_b1OvwQ&t=1612s](https://www.youtube.com/watch?v=NWqy_b1OvwQ&t=1612s)


<p align="center">
  <img alt="img-name" src="{{ site.baseurl }}/images/Hilton.png" height="auto" width="600">
    <em>Hilton is thinking, generated by DELLE 2, though not exactly the same as I want</em>
</p>

Here is a timeline about this video:

2.11: **why looking for something Beyond back propagation despite its tremendous success** 

Back propagation is a way of how much a change in the weight would make the system have less error and then you change the weight in proportion to how much it helps and obviously it hurts if you change it in the opposite direction. To calculate this weight change, it has to use the same connectivity pattern with the same weights but in the backwards direction and it has to go backwards through the non-linearity of the neuron. There's no evidence that the brain is doing that. **The worst case** is that if you're doing back propagation in a recurrent net, because you need tp run the recurrent net forwards in time and then you have to run it backwards through time in order to get all these derivatives so as to change the weights. That's particularly problematic if for example if you're trying to process video because you can't stop and go backwards in time. So combined with the fact that there's no good evidence the brain does it, and there's the problem that for technology it's a mess it interrupts the pipelining of stuff through. So you'd really like something like video there's been multiple stages of processing and you'd like to just pipeline the inputs through those multiple stages and just keep pipelining it through.

5.16: **Explanation of the forward forward algorithm** 

and so the idea of the Ford algorithm
is that if you can divide
the learning the process of getting the
gradients you need into two separate
phases you can do one of them online and
one of them offline
and the way you do online can be very
simple and will allow you to just
pipeline stuff through
so the online phase which is meant to
correspond to wake
you put input into the network
and let's take the recurrent version
input keeps coming into the network
and what you're trying to do
for each layer at each time step
you're trying to make
the layer of high activity
or rather high enough activity so that
it can
figure out that this is real data
so the underlying idea is for Real data
you want every layer to have high
activity and for fake data what comes
out we get that later you'd like every
layer to have low activity
and the task of the network or the thing
it's trying to achieve
is not to give the correct label as in
back propagation is trying to achieve
this property but being able to tell the
difference between real data and fake
data at every layer by each layer having
high activity for real data and no
activity for fake data
so each layer has its own objective
function
in fact to be more precise we take the
sum of the squares of the activities of
the units in a layer
we subtract off some thresholds
and then we feed that to a logistic
function that simply decides what's the
probability that this is a is real data
as opposed to fake data
and if the logistical function gets a
lot of input it will say it's definitely
real data
and so there's no need to change
anything if it's getting lots of input
you won't learn on that example because
it's already getting it right
and that explains how you can run lots
of positive examples without running any
negative examples which are fake data
because it'll just saturate on positive
examples it's getting right
so that's what it does in the positive
phase it tries to get high sum of
squared activities in every layer so
that it can tell high enough so it can
tell that it's real data
in the negative phase
which is run Offline that is during
sleep
the network needs to generate its own
data
and try and get given its own data as
input
it wants to have low activity in every
layer
so the network has to learn a generative
model
and what it's trying to do is
discriminate between real data and fake
data produced by its generative model
obviously if it can't discriminate at
all
then what's going to happen is the
derivatives that it gets for real data
and the derivatives we get for fake data
will be equal and opposite so it won't
learn anything learning will have
finished then if you can't tell the
difference between what it generates and
real data
this is very like again if you know
about generative adversarial Networks
except that the discriminative net
that's trying to tell the difference
between real and fake and the generative
model that's trying to generate fake
data use the same hidden units and so
they use the same hidden representations
that overcomes a lot of the problems
that a gun has
on the other hand because it's not doing
back propagation to learn the generative
model it's harder to learn a good
General model
that's a rough overview of the algorithm
