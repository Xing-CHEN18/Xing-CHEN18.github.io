---
published: true
---
Here's a nice video about interview of Geoffrey Hilton on his new Forwarad Forward algorithm, which I am currently working on based on his preliminary research. This interview is pretty cool as it shows us how a great idea was generated: thousands of times of trying, large amount of reading, consistent curiosity and explorations... and also hard working. "To have a great idea have a lot of them!" As Thomas Edison said. 

Video link: [https://www.youtube.com/watch?v=NWqy_b1OvwQ&t=1612s](https://www.youtube.com/watch?v=NWqy_b1OvwQ&t=1612s)

The first half of this video can be pretty technique, and pre-reading of his paper ([https://arxiv.org/abs/2212.13345](https://arxiv.org/abs/2212.13345)) is highly recommended to get better understanding of the talk.

<p align="center">
  <img alt="img-name" src="{{ site.baseurl }}/images/Hilton.png" height="auto" width="600">
    <em>Hilton is thinking, generated by DELLE 2, though not exactly the same as I want</em>
</p>

Here is a timeline about this video:

2.11: **why looking for something Beyond back propagation despite its tremendous success** 

Back propagation is a way of how much a change in the weight would make the system have less error and then you change the weight in proportion to how much it helps and obviously it hurts if you change it in the opposite direction. To calculate this weight change, it has to use the same connectivity pattern with the same weights but in the backwards direction and it has to go backwards through the non-linearity of the neuron. There's no evidence that the brain is doing that. **The worst case** is that if you're doing back propagation in a recurrent net, because you need tp run the recurrent net forwards in time and then you have to run it backwards through time in order to get all these derivatives so as to change the weights. That's particularly problematic if for example if you're trying to process video because you can't stop and go backwards in time. So combined with the fact that there's no good evidence the brain does it, and there's the problem that for technology it's a mess it interrupts the pipelining of stuff through. So you'd really like something like video there's been multiple stages of processing and you'd like to just pipeline the inputs through those multiple stages and just keep pipelining it through.


5.16: **Explanation of the forward forward algorithm** 

The idea of the forward forward algorithm is that if you can divide the learning process of getting the
gradients you need into two separate phases, you can do one of them online and one of them offline. The way you do online can be very simple and will allow you to just pipeline stuff through so the online phase which is meant to correspond to wake where you put input into the network. let's take the recurrent version as an example: input keeps coming into the network and what you're trying to do for each layer at each time step is you're trying to make the layer of high activity or rather high enough activity so that it can figure out that this is real data so the underlying idea is, for Real data you want every layer to have high activity and for fake data what comes out we get that later you'd like every layer to have low activity, and the task of the network or the thing it's trying to achieve is not to give the correct label (as in back propagation is trying to achieve this property) but being able to tell the difference between real data and fake data at every layer by each layer having high activity for real data and no activity for fake data so each layer has its own objective function. 


In fact, & to be more precise we take the sum of the squares of the activities of the units in a layer, we subtract off some thresholds and then we feed that to a logistic function that simply decides what's the probability that this is a real data as opposed to fake data and if the logistical function gets a lot of input it will say it's definitely real data and so there's no need to change anything, if it's getting lots of input you won't learn on that example because it's already getting it right and that explains how you can run lots of positive examples without running any negative examples which are fake data because it'll just saturate on positive examples it's getting right. So that's what it does in the positive phase where it tries to get high sum of squared activities in every layer so that it can tell that it's real data, and in the negative phase which is run Offline (that is during sleep) the network needs to generate its own data and try to give its own data as input, where it wants to have low activity in every layer. So the network has to learn a generative model and what it's trying to do is to discriminate between real data and fake data produced by its generative model. Obviously if it can't discriminate at all then what's going to happen is the derivatives that it gets for real data and the derivatives we get for fake data will be equal and opposite so it won't learn anything, and learning will have finished. Then if you can't tell the difference between what it generates and real data, this is very like again if you know about generative adversarial Networks (GAN) except that the discriminative net that's trying to tell the difference between real and fake and the generative model that's trying to generate fake
data use the same hidden units and so they use the same hidden representations that overcomes a lot of the problems that a GAN has. On the other hand because it's not doing back propagation to learn the generative
model it's harder to learn a good General model. That's a rough overview of the algorithm.

9.13: **Cyclying between wake/sleep or?** 

Most of the research what I would do is the preliminary research. Cyclying quickly between them because that's the obvious thing to do and later on I discovered well I've known for some time that with contrastive learning you can separate the phases and later on I discovered it worked pretty well to separate the phases.
Recent experiments I've done with predicting characters. You can have it predict about a quarter of a million characters when it's running on real data and trying to predict the next character is making predictions. And it's running with mini batches, so after making quite a large number of predictions they're going to updates the weights and then it sees more positive examples it updates weights again, so in all those phases it's just trying to get higher activity in the hidden layers but only if it's not already got high activity, and you can predict like quarter of a million characters in the positive phase and then switch to the negative phase where the Network's generating its own string of characters and you're now trying to get low activity in the hidden layers for the characters it's predicting. It's looking at a little window characters and then you run for quarter of a million characters like that and it doesn't actually have to be the same number anymore.  In Boltzman machines it's very important to have the same number of things in the positive phase and negative phase but with this it isn't. The most remarkable is that up to a few hundred thousand predictions it works almost as well if you separate the phases as opposed to interleave and that's quite surprising.

11.08: **How this wate/sleep phase relate to human learning** 

In human learning, we can sleep for complicated Concepts that you're learning but there's learning going on all the time that doesn't require a sleep phase well. There is in this too, if you're just running on positive examples it's changing the weights for all the examples where it's not completely obvious that this is a positive data so it does a lot of learning in the positive phase but if you go on too long you fails catastrophically and people seem to be the same if probably sleep for a week you'll go completely psychotic and you may never recover.


12.17: **How to understand the negative data** 

























